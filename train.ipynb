{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc04bd6a-5230-4ad5-a902-6df9032dabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100bda12-41f4-40f3-950f-0a0439a95d9c",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b61a72-a2d4-408c-b153-ed5e6290d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ashaba1in/small_openwebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "598630b0-586d-41c1-a9ec-182d41f62189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\\n\\nThe decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\\n\\nCNN initially reported, based on conversations with some of the doctors, that the United Nations ordered the Belgian First Aid and Support Team to evacuate. However, Belgian Chief Coordinator Geert Gijs, a doctor who was at the hospital with 60 Belgian medical personnel, said it was his decision to pull the team out for the night. Gijs said he requested U.N. security personnel to staff the hospital overnight, but was told that peacekeepers would only be able to evacuate the team.\\n\\nHe said it was a \"tough decision\" but that he accepted the U.N. offer to evacuate after a Canadian medical team, also at the hospital with Canadian security officers, left the site Friday afternoon. The Belgian team returned Saturday morning.\\n\\nGijs said the United Nations has agreed to provide security for Saturday night. The team has requested the Belgian government to send its own troops for the field hospital, which Gijs expects to arrive late Sunday.\\n\\nResponding to the CNN report that Gupta was the only doctor left at the Port-au-Prince field hospital, U.N. spokesman Martin Nesirky said Saturday that the world body\\'s mission in Haiti did not order any medical team to leave. If the team left, it was at the request of their own organization, he said.\\n\\nEdmond Mulet, the U.N. assistant secretary general for peacekeeping operations, told reporters later that local security officers deemed the makeshift hospital unsafe.\\n\\n\"It seems that we\\'ve heard some reports in the international media that the United Nations asked or forced some medical teams to not work any more in some clinic -- that is not true, that is completely untrue,\" Mulet said Saturday.\\n\\nCNN video from the scene Friday night shows the Belgian team packing up its supplies and leaving with an escort of blue-helmeted U.N. peacekeepers in marked trucks.\\n\\nView or add to CNN\\'s database of missing persons in Haiti\\n\\nGupta -- assisted by other CNN staffers, security personnel and at least one Haitian nurse who refused to leave -- assessed the needs of the 25 patients, but there was little they could do without supplies.\\n\\nMore people, some in critical condition, were trickling in late Friday.\\n\\n\"I\\'ve never been in a situation like this. This is quite ridiculous,\" Gupta said.\\n\\nWith a dearth of medical facilities in Haiti\\'s capital, ambulances had nowhere else to take patients, some of whom had suffered severe trauma -- amputations and head injuries -- under the rubble. Others had suffered a great deal of blood loss, but there were no blood supplies left at the clinic.\\n\\nGupta feared that some would not survive the night.\\n\\nHe and the others stayed with the injured all night, after the medical team had left and after the generators gave out and the tents turned pitch black.\\n\\nGupta monitored patients\\' vital signs, administered painkillers and continued intravenous drips. He stabilized three new patients in critical condition.\\n\\nAt 3:45 a.m., he posted a message on Twitter: \"pulling all nighter at haiti field hosp. lots of work, but all patients stable. turned my crew into a crack med team tonight.\"\\n\\nAre you in Haiti and safe? Share your photos\\n\\nHe said the Belgian doctors did not want to leave their patients behind but were ordered out by the United Nations, which sent buses to transport them.\\n\\n\"There is concern about riots not far from here -- and this is part of the problem,\" Gupta said.\\n\\nThere have been scattered reports of violence throughout the capital.\\n\\n\"What is striking to me as a physician is that patients who just had surgery, patients who are critically ill, are essentially being left here, nobody to care for them,\" Gupta said.\\n\\nSandra Pierre, a Haitian who has been helping at the makeshift hospital, said the medical staff took most of the supplies with them.\\n\\n\"All the doctors, all the nurses are gone,\" she said. \"They are expected to be back tomorrow. They had no plan on leaving tonight. It was an order that came suddenly.\"\\n\\nShe told Gupta, \"It\\'s just you.\"\\n\\nA 7.0 magnitude earthquake flattened Haiti\\'s capital city Tuesday afternoon, affecting as many as 3 million people as it fanned out across the island nation. Tens of thousands of people are feared dead.\\n\\nHaiti, the poorest nation in the Western hemisphere, lacked adequate medical resources even before the disaster and has been struggling this week to tend to huge numbers of injured. The clinic, set up under several tents, was a godsend to the few who were lucky to have been brought there.\\n\\nRetired Army Lt. Gen. Russel Honore, who led relief efforts for Hurricane Katrina in 2005, said the evacuation of the clinic\\'s medical staff was unforgivable.\\n\\n\"Search and rescue must trump security,\" Honoré said. \"I\\'ve never seen anything like this before in my life. They need to man up and get back in there.\"\\n\\nHonoré drew parallels between the tragedy in New Orleans, Louisiana, and in Port-au-Prince. But even in the chaos of Katrina, he said, he had never seen medical staff walk away.\\n\\n\"I find this astonishing these doctors left,\" he said. \"People are scared of the poor.\"\\n\\nCNN\\'s Justine Redman, Danielle Dellorto and John Bonifield contributed to this report.',\n",
       " 'Former secretary of state Hillary Clinton meets voters at a campaign rally in St. Louis on Saturday. (Melina Mara/The Washington Post)\\n\\nDemocratic front-runner Hillary Clinton was ahead by a slim margin in Missouri on Wednesday, but the race remained in limbo pending word on whether rival Sen. Bernie Sanders of Vermont would seek a recount.\\n\\nThe delay postponed a definitive answer to whether Clinton had made a clean sweep of five big primaries on Tuesday night. Even if she does not prevail in Missouri, her other victories push her closer to the Democratic presidential nomination even as the considerably weakened Sanders vowed to press on with his insurgent campaign.\\n\\nClinton won big in Florida, North Carolina and Ohio, while claiming a narrower victory in Illinois. In Missouri, with 100 percent of precincts reporting, Clinton was ahead 310,602 votes to 309,071. With a difference of less than 1 percent, state officials held off calling the race. A recount is not automatic, but Sanders could request one.\\n\\nDemocratic front-runner Hillary Clinton and her rival, Bernie Sanders, spoke about the challenges going forward after primary voters took to the polls in five states on March 15. (Sarah Parnass/The Washington Post)\\n\\nJeff Weaver, Sanders’s campaign manager, said the campaign has not made a final decision on whether to request a recount and is still looking at the numbers. Because delegates are awarded proportionately, it’s not clear how much a small change in the vote totals would matter, he said.\\n\\n“If it’s not going to make a material difference in the delegate count, we’re not going to put people through it,” he said.\\n\\n[A good night for Trump and a better night for Clinton]\\n\\nClinton campaign manager Robby Mook issued a memo to supporters and the media Wednesday that claimed a decisive advantage. He also took Sanders to task for turning negative.\\n\\n\"Both campaigns agreed that the measure of success for yesterday\\'s pivotal contests was delegates,\" Mook wrote. \"Sanders went all out in these 5 states, pouring more than $8 million on TV in the last 5 days alone,\" including at least one ad Mook termed negative.\\n\\n\"It\\'s pretty clear this negative strategy backfired,\" he wrote.\\n\\nAddressing supporters Tuesday night, Sanders did not mention the night\\'s outcome, a disappointment for him after hopes that he could ride momentum from an upset victory in Michigan last week to victories in other large, delegate-rich states in the Midwest.\\n\\nIn a statement issued overnight, Sanders congratulated Clinton and pledged to continue a primary fight that he said he is confident he can still win. He did not mention Missouri or the other contests by name.\\n\\n\"With more than half the delegates yet to be chosen and a calendar that favors us in the weeks and months to come, we remain confident that our campaign is on a path to win the nomination,\" Sanders said.\\n\\nBut that path looked much more difficult, if not impossible, on Wednesday. Clinton\\'s victories set her more than 300 delegates ahead of Sanders, and she is on track to collect a large share of the more than 1,000 delegates she still needs to lock up the contest. Sanders ended the day further behind in the delegate count — and needing to win a slew of upcoming states by improbably large margins.\\n\\n“We are moving closer to securing the Democratic Party nomination and winning this election in November,” Clinton said at her victory party here Tuesday. As if to prove the point, she quickly pivoted to the Republican front-runner, Donald Trump.\\n\\n“Our next president has to be ready to face three big tasks,\" Clinton said during a speech that looked past her primary fight with Sanders and ahead to a probable matchup with Republican front-runner Donald Trump.\\n\\n\"First, can you make positive differences in people’s lives? Second, can you keep us safe? Third, can you bring our country together again?”\\n\\nClinton’s indictment of Trump’s policy positions sounded like a preview of arguments to come.\\n\\n“When we hear a candidate for president call for the rounding up of 12 million immigrants, banning all Muslims from entering the United States, when he embraces torture, that doesn’t make him strong, it makes him wrong,” Clinton said.\\n\\nClinton has been eager to refocus her campaign to confront Trump more directly. But asked Tuesday if she was concerned that a protracted primary fight with Sanders would hobble Democrats ahead of the contest against a Republican nominee, she declined to encourage Sanders to leave the race.\\n\\nHer campaign emailed a fundraising pitch Tuesday evening warning of the dangers of a Trump presidency and of complacency among Democrats.\\n\\n“Tonight, Donald Trump could become the presumptive Republican nominee for president,” the donation request began. Too many Republicans tried to ignore him until it was too late, it said.\\n\\nSanders held a rally before about 7,000 people in Phoenix on Tuesday night, a week ahead of Arizona’s primary.\\n\\nHe said his campaign had “defied all expectations” but made no mention of the three states that had already been called in Clinton’s favor.\\n\\n“What excites me so much as I go around the country is to see the incredible energy of people who love this country but know we can do so much better,” Sanders said to loud screams.\\n\\nSome of his die-hard supporters expressed hope that he could still pull out the nomination.\\n\\n“I still think the revolution is coming,” said James Homan, 55, a sound engineer for rock musicians, who has homes in Illinois and Arizona.\\n\\nHoman expressed frustration that, as he saw it, “the fix was in” for Clinton among Democratic Party leaders, but he said he could see paths for Sanders to prevail, including the possibility of more fallout from the FBI investigation into Clinton’s use of a private email server while she was secretary of state.\\n\\nDemocratic primary voters were split on the candidates’ key attributes, with Clinton seen as more electable and Sanders as more honest, according to preliminary exit polls reported by ABC News.\\n\\nBy roughly 2 to 1, voters across Ohio, North Carolina, Florida, Illinois and Missouri said Clinton had a better chance than Sanders of beating Trump in a general-election matchup. But roughly 8 in 10 said Sanders was honest and trustworthy, compared with about 6 in 10 who felt that way about Clinton. Sanders has dominated among honesty-focused voters all year, while Clinton has won by a wide margin those who care more about electability.\\n\\nSanders had embarrassed Clinton last week in Michigan and saw Tuesday’s contests as a chance to pull off more come-from-behind wins in states where voters feel damaged by globalization.\\n\\nRepeating his playbook from Michigan, Sanders hit Clinton hard on her past support for “disastrous” trade deals, starting with the North American Free Trade Agreement when her husband was in the White House.\\n\\nWith the lesson of Michigan in mind, her campaign moved to retool her stance on trade by strengthening her opposition to the Trans-Pacific Partnership and emphasizing support for manufacturing in her jobs plan. In Ohio, Clinton took specific aim at elements of the pending trade package seen as harmful to the auto and steel industries.\\n\\nJust over half of Ohio Democratic primary voters said free trade takes away U.S. jobs, according to the early exit polls. In Michigan, Sanders won among voters with that view by double digits. The anti-trade cohort was slightly larger in Michigan (57 percent) than in most states voting Tuesday, with less than half of Democrats in Illinois, Missouri and North Carolina saying trade costs U.S. jobs.\\n\\nIn Youngstown, Ohio, Dave Williams, 52, cast a ballot for Sanders.\\n\\n“I lost my house when the stock market crashed,” said Williams, a member of the local cement finishers union. “I’m an angry voter, how ’bout that? I’m angry about the way the country is working for the blue-collar worker. Hillary gets a big, fat zero on that.”\\n\\nIn Missouri, Sanders aides were optimistic in part because much of the state closely resembles Kansas, where the senator easily defeated Clinton in the Democratic caucuses early this month. It’s worth noting, however, that Missouri was the smallest of the Democratic delegate prizes Tuesday.\\n\\nBefore the polls closed in Missouri, Clinton’s campaign announced that she had been endorsed by the mother of Michael Brown, the teenager whose 2014 shooting by police in Ferguson, Mo., brought more attention to officer-involved slayings of unarmed black men.\\n\\nIn Chicago, where Clinton spent her childhood, Sanders sought to leverage support from voters disenchanted with the tenure of the city’s embattled Democratic mayor, Rahm Emanuel, a Clinton ally. Emanuel’s approval ratings have dropped to all-time lows amid controversies over a police shooting and school closings, and his popularity with African American voters has taken an especially big hit.\\n\\nIn the closing days of the race, Sanders blasted Emanuel’s decision to close schools in predominantly black and Latino neighborhoods, and Sanders ran television ads featuring some of the mayor’s critics.\\n\\nAnd Tuesday, Sanders had breakfast with Cook County Commissioner Jesús “Chuy” García, who ran unsuccessfully for mayor against Emanuel in the Democratic primary last year.\\n\\nClinton’s lead in Florida was never in doubt, and she ended up capturing almost the same number of votes as the Republican winner, Trump — perhaps a preview of how competitive the state will be in November.\\n\\nFlorida posed several challenges for Sanders. It held a closed primary, meaning independent voters, who have propelled him to victory in other states, were not allowed to participate. The state’s voting population also includes a large number of older voters, who have sided with Clinton in previous contests.\\n\\nSanders’s aides have argued that the back half of the nominating calendar is more favorable to him, with several potential victories in the West and no contests remaining in the Deep South, which has been Clinton’s strongest region by far.\\n\\nSanders thinks he is well-positioned in all three states with contests next Tuesday: Arizona, Idaho and Utah. His decision to spend election night in Arizona signaled his intention to vigorously contest that state in the coming week.\\n\\nScott Clement contributed to this report.',\n",
       " 'The opinions expressed by columnists are their own and do not represent the views of Townhall.com.\\n\\nYou have to give President Barack Obama credit for one thing: consistency. Nothing is ever his fault. Nothing will ever be his fault. Faulting Fox News and the American people, on the other hand, now that\\'s a different story.\\n\\nDo you remember when Obama traipsed around the country and desperately pleaded with Americans to vote for Hillary Clinton because his agenda and his legacy were on the ballot? He made a similar pitch before the shellacking his party took in the 2014 congressional elections.\\n\\nYet did he acknowledge after this 2014 failing that he had anything to do with it? Does he own up to his leading role in last month\\'s presidential election?\\n\\nLet\\'s rewind the tape further, to Obama\\'s reaction to his party\\'s stunning defeat in the 2010 congressional elections, which was largely about Obamacare. He didn\\'t acknowledge any personal culpability for visiting that monstrosity on the American people through trickery and deceit. He simply lamented that he hadn\\'t done a good enough job getting the message out to the American people about it, despite his 50 propaganda speeches trying to persuade us to ignore our lying eyes.\\n\\nDo you see the pattern here? Obama\\'s view is that the American people -- those in the red states, anyway -- are a little slow, paranoid and bigoted and need to be brought along carefully into the 21st century, where progressivism has ushered in a new age of enlightenment. His only failing has been in inadequately re-educating the bitter clingers.\\n\\nLet me give you another example. Remember Obama\\'s depiction of the Islamic State group as \"a JV team\"? How about his claim, the night before the terrorist massacres in Paris, that the Islamic State was \"contained\"?\\n\\nDid he ever acknowledge his errors there? No. Again, his only failing was in not having communicated sufficiently his counterterrorism strategies to the American people. He said his strategy against the Islamic State was working. (This was before, as I recall, his admission that he had no policy.) The problem was that saturated media coverage after the Paris attacks was fueling terror fears in the United States. He said: \"We haven\\'t, on a regular basis, I think, described all the work that we\\'ve been doing for more than a year now to defeat\" the Islamic State. \"If you\\'ve been watching television for the last month, all you\\'ve been seeing, all you\\'ve been hearing about is these guys with masks or black flags who are potentially coming to get you. And so I understand why people are concerned about it.\"\\n\\nAgain, there\\'s nothing to see here. It\\'s not a terrorism problem but a perception problem. There\\'s no Obamacare problem; it\\'s just that the American people don\\'t get it.\\n\\nEven liberal New York Times columnist Maureen Dowd acknowledged, in 2012, that Obama and his wife, Michelle, are condescending and aloof. The Obamas \"do believe in American exceptionalism -- their own, and they feel overassaulted and underappreciated,\" she wrote. The Obamas haven\\'t disappointed Americans; \"we disappointed them.\"\\n\\nEven earlier, in February 2010, Obama pledged to \"listen\" to Republicans at a health care summit. But, as columnist Joseph Curl wrote, \"turns out he meant he\\'d be listening to his own voice. By the end of the televised event, Mr. Obama had spoken for 119 minutes -- nine minutes more than the 110 minutes consumed by 17 Republicans. The 21 Democratic lawmakers used 114 minutes, giving the president and his supporters a whopping 233 minutes.\"\\n\\nAnd why do the rubes keep misperceiving Obama\\'s greatness? Fox News, Rush Limbaugh, Sean Hannity.\\n\\nIn a recently published interview with Rolling Stone, Obama denied that he and his party overlooked the \"cohort of working-class white voters\" that supposedly accounted for Donald Trump\\'s victory. Absolutely not his fault. \"Part of it,\" said Obama, \"is Fox News in every bar and restaurant in big chunks of the country, but part of it is also Democrats not working at a grass-roots level, being in there, showing up, making arguments.\"\\n\\nThe challenge Democrats have, according to Obama, is not that they\\'ve neglected these communities from a policy perspective. \"What is true, though, is that whatever policy prescriptions that we\\'ve been proposing don\\'t reach, are not heard by, the folks in these communities. And what they do hear is \\'Obama or Hillary are trying to take away (your) guns\\' or \\'they disrespect you.\\'\"\\n\\nI repeat: This guy is remarkably, incorrigibly consistent. He has made no policy errors; his message just isn\\'t getting through, partly because the conservative media are lying about it and partly because people are just too darned dense.\\n\\nI hate to keep bringing up the past, but his war on the conservative media is nothing new, either. I wrote about it in 2010 in my book \"Crimes Against Liberty.\" He began snubbing Fox reporters at news conferences for insufficiently pandering. The White House blog regularly denounced Fox News and other critics. White House communications director Anita Dunn recommended a \"rapid response\" to counteract \"Fox\\'s blows\" against the administration, calling Fox News \"part of the Republican Party.\" Presidential adviser David Axelrod said Fox News Channel is \"not really a news station.\"\\n\\nRemember when Obamacare\\'s principal architect, Jonathan Gruber, openly admitted that the Obama administration was able to deceive the American people about Obamacare and chalked it up to \"the stupidity of the American voter\"?\\n\\nSo go ahead and cry us a river about how the conservative media are mistreating you, Mr. Obama, and misleading the public. You have been trying to deceive us for eight years, and the public has been onto you for at least 6 1/2 of those years. Now voters have handed you your biggest spanking yet, and you still will not listen. You can\\'t listen. It\\'s not what you do. But the American people have been listening, and they do understand your policies. And it\\'s a new day in America.',\n",
       " \"BIGBANG is one of those musical entities that transcends language. It’s one of those rare groups that both innovates and defines the direction a genre takes. Covering a sound that includes hip hop, R&B and electronic dance, BIGBANG and its solo acts (G-Dragon, T.O.P, Taeyang, Seungri and Daesung) have left a musical imprint that has affected the global music market. In fact, even Diplo, a household name in EDM, worked with G-Dragon and T.O.P for their rap album. So when the band announced its world tour to promote the release of its third full-length studio album MADE after a 3 year hiatus, fans lost their minds – including myself. In fact, tickets for each of BIGBANG’s North American legs sold out.\\n\\nAs a result, I was lucky enough to witness this larger-than-life Korean pop group perform a couple Saturday nights ago on Oct. 10 at the Prudential Center in Newark, NJ. As I waited in line to enter the venue with my friends, mobs of fans raved about BIGBANG's new tracks (and surprisingly, not everyone was Asian). Leading up to this particular leg of their North American tour, BIGBANG released 2 songs every month starting from May to August, resulting in 8 freshly minted tracks.\\n\\nAfter everyone pushed their way through security, a slew of fans rushed to the merchandise table hoping to get either apparel or light up accessories they could wave around during the concert. Being the broke boy I am, my friend and I instead made our way to our seats at the front of the upper level and waited for the BANG to make its appearance. (Floor admission was anywhere from $600 - $800.)\\n\\nIn the hour leading up to BIGBANG’s presence, a large screen played popular music videos from both the group as a whole and its solo acts. Though the pit was half full and other fans were sparsely scattered in the seated areas, fans emphatically cheered when their favorite idol appeared in a music video. Prudential Center wasn’t even a quarter full yet.\\n\\nAs the venue slowly flooded to capacity, the group finally made its appearance. Gradually, the lights dimmed while the sound of a motor revving filled the venue. Fans energetically waved their light-up flower accessories in anticipation and stood up. Almost immediately, the instrumentals to “BANG BANG BANG” blared through the speakers, and the 5 members walked out of the splitting screen previously used to play the videos. During the song, small fireworks burst from the top of the stage and popped timely on the hook’s lyric “bang.” It was then that everyone recognized the blissful hype BIGBANG was about to deliver for the rest of the night on.\\n\\nAs their set progressed, the band showcased their sentimental side, performing a set of slow ballads at once to drive home the theme of their first album single “LOSER,” yet also re-energized the crowd with another set of R&B and pop songs. In between, as if to both recharge their energy and quickly change into their costume-like clothes, each artist performed a song from his solo work. This break from the group’s work gave more casual fans a chance to witness how the band dedicated its time during its hiatuses.\\n\\nThe ultimate highlight came during T.O.P’s solo “DOOM DADA.” Adorning a suit printed with Mondrian’s patented “Composition” painting, instead of rapping his line, T.O.P looked at the audience and winked. And during that moment, my jaw dropped, and I questioned my sexuality.\\n\\nIn between a set of maybe three or four songs, the band either took the time to interact with the audience or reveal extended scenes from their Quentin Tarantino-inspired short film uploaded in April.\\n\\nIt was when the band members spoke to the audience that these Korean idols brought themselves down to earth. Despite occasionally tripping over their rehearsed English lines, each member gave us a glimpse of his more personal side. Some had further solidified their reputations. For instance, Daesung, the goofy, go-lucky singer and drummer, introduced himself just by yelling “YEAH” to the audience at least 5 times and waited for the audience’s response between each. T.O.P re-asserted himself as the cool, collected bad boy, asserting, “Yeah… You know who I am” when it was his turn at the microphone.\\n\\nThe others seemed to break out of fan expectations. While Seungri was the most mysterious before the concert, known more for his dancing than his singing, he ended up being the most charismatic of the group and spoke the most English. Taeyang also shed the hip hop image he tries to convey in his videos when he’s clad in Supreme. His initial interaction with the audience was inviting the fans to sing pitches with him, and later on, he would be the one to interject between the other members’ later interactions with the audience. And while G-Dragon is the eclectic leader of the group, he was surprisingly toned down compared to the chameleon style that has garnered him the attention of fashionheads everywhere.\\n\\nThe biggest surprise, though, was the group’s capacity for profanity. In an attempt to hype up the audience for his solo “Strong Baby,” Seungri yelled at the audience to “MAKE SOME FUCKING NOISE.” For his part in “Zutter,” T.O.P repeatedly rapped “bitch” despite his line only requiring him to say it once. And at the end, G-Dragon introduced the final song as “FANTASTIC MOTHERFUCKING BABY.”\\n\\nOf course, there are more moments that I could cover – like how Daesung made a scene about revealing his hair-covered eyes, or how he pelvic thrusted to Michael Jackson’s “Billie Jean,” or how T.O.P still uncoordinatedly dances in the back – but these moments are boring to read and seem underwhelming on paper.\\n\\nIf there’s one thing to take away, it’s that this concert was an electric, out-of-body experience. I had the privilege of being in the presence of some of my favorite artists, foolishly screaming and singing and dancing along to some of my favorite songs live — all without the influence of drugs and alcohol. Despite their huge break, this comeback concert only proved that BIGBANG’s presence is stronger than ever and cemented my opinion that they’re the absolute best in Korean pop. Unfortunately, as the concert concluded, the very real idea that each member would undergo required conscription loomed closer. But it didn’t matter — if this was BIGBANG’s last world tour, it ended perfectly. If there is another years from now, though, I can’t wait to be there too.\",\n",
       " 'WHAT?!??! I know. That’s what you’re saying right now.\\n\\n“WHAT?! DISNEY HAS A DONUT SUNDAE AND I DIDN’T KNOW ABOUT IT?!” How do I know you’re saying that? Because that’s exactly what I was saying when Tina (<– amazing DFB photographer who’s frigging awesome) spotted this at the Plaza Ice Cream Parlor in the Magic Kingdom this week!\\n\\nBut it’s OK. It’s brand new — not even on the menu yet — so we didn’t miss out on too much of the Donut Sundae lifespan. And we’re hoping that lifespan is a nice, long one!\\n\\nThe Main Street Plaza Ice Cream Parlor can be found at the intersection of Main Street USA and Tomorrowland, just before you get to Cinderella Castle. And the sundae joins a few other must-have treats on the Ice Cream Parlor’s menu, including the house-made ice cream sandwich (preferably ordered with a drizzled sauce!), the “kids’ cone” (it’s totally OK to order this as a grown-up, too) with Mickey ears, and the Plaza Ice Cream Sundae. So…I’m really not envying you the decisions you’ll have to make when you get there! ;-D\\n\\nAfter spotting the sundae on a placard, we grabbed it! It comes with a warm glazed donut, warm apple compote, vanilla ice cream, chocolate sauce, whipped cream, a cherry, chocolate chips, and peanut butter chips.\\n\\nWe found out that the donut was not house-made (it’s basically a Krispy Kreme), but it’s warmed just before serving. And with the warm donut and warm compote contrasting with the cold vanilla ice cream, there’s a LOT of amazing going on in this sundae!\\n\\nI’m doubting the apple compote is house-made, or even fresh (probably canned), but it still works well with the sundae. Though the combo of apple compote with peanut butter chips is a bit strange?\\n\\nAt the moment, vanilla ice cream is the default flavor. The Cast Member we spoke to said that once this is officially on the menu, guests should be able to request whatever flavor they’d like.\\n\\nAt press time, this costs $5.99 and is not a Disney Dining Plan snack credit option. But at $5.99, you’re still getting a bargain as far as we’re concerned!\\n\\nOur thoughts? This was fantastic! Donut? Good! Ice Cream? Good! Apple Pie Filling? Good! Whipped Cream? Good! It’s a winner all around. We can’t WAIT until this gets cemented onto the menu!\\n\\nPin it for later!\\n\\nWhat do you think? Will you be heading to the Plaza Ice Cream Parlor for your Donut Sundae the next time you’re on Main Street USA? Let us know in the comments below!',\n",
       " 'A notorious protester convicted of wilfully promoting hatred against Muslims and criminally harassing a Muslim man and his family was sentenced Tuesday to nine months in jail. Eric Brazau handed out a flyer that “vilified Muslims and disparages their religion,” Ontario court Judge S. Ford Clements said in February, when he found Brazau guilty.\\n\\nEric Brazau was convicted of willful promotion of hatred against Muslims and criminally harassing a Muslim family. ( CARLOS OSORIO / TORONTO STAR FILE PHOTO )\\n\\nThe case was far from being on the borderline between “rough and tumble debate” and hate speech, as Brazau had argued, Clements said in a College Park courtroom. Brazau handed out the flyer, which contained many offensive references to Islam and Muslims, in August and September 2012. While distributing it, Brazau sometimes yelled obscenities about Islam “in a tone of voice that suggested he was very angry and had little interest in debate,” Clements said. Brazau had argued that he did not intend to promote hate speech; instead he wanted to stimulate debate about censorship, “blasphemy laws” and Sharia law, Clements said.\\n\\nArticle Continued Below\\n\\nClements disagreed. “He knew the material would deeply wound and anger Muslims,” said Clements. The content was not humorous, ironic or satirical, he said. “Mr. Brazau is far too intelligent to believe this to be so.” The flyer also contained a somewhat blurred photograph of a Muslim family on a downtown Toronto street.\\n\\nThe man in the photo testified that Brazau called him a “terrorist” on the day Brazau took the photo. In a second interaction a few weeks later on a sidewalk, the man, whose name is protected by a publication ban, said that Brazau approached him aggressively while photographing the family, making him “concerned and fearful.”\\n\\nArticle Continued Below\\n\\nClements found this to be criminal harassment. During sentencing submissions, Crown prosecutor Derek Ishak described Brazau as an “unrepentant hatemonger … who abused his right to freedom of speech in a planned, deliberate manner,” Clements said Tuesday in his sentencing decision. However, Clements said that while Brazau’s conduct was “despicable” and his beliefs “repugnant,” the maximum sentence of six months for a summary conviction on willfully promoting hatred was unwarranted. He also noted the defence submission that Brazau committed his offences in public, where he was easily identifiable, rather than by stealth. Instead, he gave Brazau a four-month sentence, plus two months for criminal harassment and mischief and three months for breach of probation by not keeping the peace. Brazau, who had spent nine months in pre-trial custody, was sentenced to time served. Clements declined to ban Brazau from distributing flyers, since that could impede his right to freedom of expression. Outside the court, Brazau said he will appeal his sentence. He says he is aware the flyer was “problematic” and “would offend.” But his voice won’t be silenced, Brazau added, though he will keep in mind the hate speech laws, which he says he has learned to navigate over the past few months. “Hatred is the harvest he wanted to gather,” Clements said in his conviction decision, quoting William Butler Yeats. “I find this is true of Mr. Brazau.” Last month, a small claims court found that Brazau had been wrongfully arrested and detained while protesting near Sgt. Ryan Russell’s funeral procession in 2011. However, the deputy judge also found his conduct “reprehensible” and awarded him only $1,000 in damages.',\n",
       " \"× Some Seattle businesses closed for ‘A Day Without Immigrants’, but others decided against it\\n\\nSEATTLE — While there is no official list of local businesses participating in this movement, “A Day Without Immigrants”, we did find some businesses that had closed their doors and posted signs up saying they were participating.\\n\\nIn fact, one business owner says he’s doing things a little bit differently. Instead of shutting down, he says he is choosing to pay it forward.\\n\\nPlease enable Javascript to watch this video\\n\\nEdward Moran moved from Mexico to the United States in 1984. He opened El Norte Lounge in Lake City about seven years ago. And instead of closing up shop along with many other immigrant small business owners across the nation, Eduardo was open for business Thursday.\\n\\n“I don’t believe in closing a place just to protest,” says Eduardo.\\n\\nInstead, he says he’s paying it forward. Eduardo is asking customers to pay in cash to avoid bank fees from credit card machines. He says he plans to donate a portion of his proceeds to a nonprofit that focuses on helping immigrants here in the Pacific Northwest. And he’s calling on other business owners to do the same.\\n\\n“If we do this together for one week, I want to see the impact we have on this economy at the bank,” says Eduardo.\\n\\n“We’re supporting him and his efforts and he's paying it forward,” says customer Kim Lawson.\\n\\n“I think it's important to honor those people who are here and working really hard,” says customer Jill Scollard.\\n\\nWhile there is no official list of businesses participating in this protest locally, we drove around Seattle tonight and found a few shops, including one on Capital Hill and one in White Center, with notices on the door saying they were shut down in support of “A day without immigrants”.\\n\\n“I did think about closing, but my heart told me and my gut instincts said you are wrong,” says Eduardo.\\n\\nWhile he knows some may criticize him for staying open, he says he’s doing what he thinks is best for his business and his employees.\\n\\n“In this industry everybody makes money every day; tips, you stop it’s not just your paycheck, it’s your tips. Share the wealth instead of stopping and not doing anything work hard and share the wealth,” says Eduardo.\",\n",
       " 'Today, Toyota announced changes in executives’ areas of responsibility, as well as personnel changes at the sub-executive managerial level. The most important change by far is the appointment of Akio Toyoda, the company’s CEO and grandson of founder Kiichiro Toyoda, as President of a new ‘EV Business Planning’ department.\\n\\nEarlier this month, we reported–admittedly a little tongue-in-cheek–about Toyota announcing the creation of an electric vehicle division and putting only 4 engineers on the project with the goal to bring EVs to market by 2020.\\n\\nThe move seems a lot more serious now that Akio Toyoda is leading the effort, and several other executives, managers, and engineers have been assigned new responsibilities in the electric vehicle planning department, including the chief engineer of the Prius.\\n\\nAt the executive level, the changes will be effective today, while the managers were apparently put on the program throughout the month. You can see the full list of changes below.\\n\\nIt appears to be a clear sign that Toyota is more serious than ever about electric vehicles and it is not simply investing in fuel cell hydrogen to comply to new fuel consumption standards.\\n\\nChanges to executives’ areas of responsibility\\n\\n(effective December 1, 2016)\\n\\nName Current New Akio Toyoda ― President EV Business Planning Dept. (chief officer) Mitsuhisa Kato Executive Vice President Frontier Research Center (chief officer) Executive Vice President Frontier Research Center (chief officer)\\n\\nEV Business Planning Dept. (chief officer) Shigeki Terashi Executive Vice President Strategic Top Executive Meeting Office (secretary general)\\n\\nCorporate Strategy Div. (chief officer)\\n\\nResearch Div. (chief officer) Executive Vice President Strategic Top Executive Meeting Office (secretary general)\\n\\nEV Business Planning Dept. (chief officer)\\n\\nCorporate Strategy Div. (chief officer)\\n\\nResearch Div. (chief officer) Koki Konishi Managing Officer Mid-size Vehicle Company (executive vice president) Managing Officer General Administration & Human Resources Group\\n\\nChanges to executive general managers’ areas of responsibility\\n\\n(effective November 1, 2016)\\n\\nName Current New Shinichi Yasui Mid-size Vehicle Company ZS (chief officer), ZV (chief officer), ZD (chief officer), ZE (chief officer), ZF (chief officer) Mid-size Vehicle Company ZS (chief officer), ZV (chief officer), ZD (chief officer), ZE (chief officer), ZF (chief officer, concurrent chief engineer)\\n\\nPersonnel changes at the sub-executive managerial level\\n\\n(effective November 1, 2016)\\n\\nName Current New Kouji Toyoshima MSZ, Mid-size Vehicle Company (chief engineer) MSZ, Mid-size Vehicle Company (chief engineer)\\n\\nEV Business Planning Dept. (preliminary organization) (general manager)\\n\\n(effective November 14, 2016)\\n\\nName Current New Kenichi Komuro Temporary External Transfer from Aisin Seiki Co., Ltd. EV Business Planning Dept. (preliminary organization) (project general manager)\\n\\n(effective December 1, 2016)',\n",
       " 'North Korean leader Kim Jong Un. AP Images / Business Insider\\n\\nNorth Korea attempted to fire a missile Sunday, but it blew up within seconds.\\n\\nIt happened one day after the anniversary of the country\\'s founding.\\n\\nWhile North Korea\\'s missile program may be the shadowiest on earth, it\\'s possible that US cyber warriors were the reason for the failed launch.\\n\\nA recent New York Times report uncovered a secret operation to derail North Korea\\'s nuclear-missile program that has been raging for at least three years.\\n\\nEssentially, the report attributes North Korea\\'s high rate of failure with Russian-designed missiles to the US meddling in the country\\'s missile software and networks.\\n\\nAlthough North Korea\\'s missile infrastructure lacks the competence of Russia\\'s, the Soviet-era missile on which North Korea based its missile had a 13% failure rate, and the North Korean version failed a whopping 88% of the time, according to the report.\\n\\nWhile the missile failure on Sunday could have just been due to poor workmanship, US Deputy National Security Adviser K.T. McFarland seemed to leave room for speculation about espionage, telling Fox News, \"We can\\'t talk about secret intelligence and things that might have been done, covert operations, so I really have no comment.\"\\n\\nVice President Mike Pence on Monday visited the demilitarized zone between the Koreas, saying that \"all options are on the table to achieve the objectives and ensure the stability of the people of this country,\" and that \"the era of strategic patience\" with North Korea \"is over.\"\\n\\nTo those in the know, the campaign against North Korea came as no surprise. Ken Geers, a cybersecurity expert for Comodo with experience in the National Security Agency, told Business Insider that cyber operations like the one against North Korea were the norm.\\n\\nWhile the US hacking another country\\'s missile program may be shocking to some, \"within military intelligence spaces, this is what they do,\" Geers said. \"If you think that war is possible with a given state, you\\'re going to be trying to prepare the battle space for conflict. In the internet age, that means hacking.\"\\n\\nReuters\\n\\nNorth Korea\\'s internal networks are fiercely insulated and not connected to the internet, however, which poses a challenge for hackers in the US. But Geers said it was \"absolutely not the case\" that hacking requires computers connected to the internet.\\n\\nA recent report in The New Yorker on Russian hacking detailed one case in which Russia gained access to a NATO computer network in 1996 by providing bugged thumb drives to shops near a NATO base in Kabul, Afghanistan. NATO operators bought the thumb drives, used them on the network, and just like that, the Russians were in.\\n\\n\"That\\'s where SIGINT (signals intelligence) or COMINT (communications intelligence) comes into collaboration with HUMINT (human intelligence),\" Geers said.\\n\\nHe described the present moment as the \"golden age of espionage,\" as cyberwarfare remains nonlethal, unattributable, and almost completely unpunished.\\n\\nBut a recent missile salvo from North Korea suggests that even a prolonged, sophisticated cyberattack can\\'t fully derail its nuclear-missile program.\\n\\n\"Imagine you\\'re the president. North Korea is a human-rights abuser and an exporter of dangerous technology,\" Geers said. \"Responsible governments really need to think about ways to handle North Korea, and one of the options is regime change.\"\\n\\nThe test fire of Pukguksong-2 in February. KCNA/Handout via Reuters\\n\\nFurther, Geers said, because of the limited number of servers and access points to North Korea\\'s very restricted internet, \"if it ever came to cyberwar between the US and North Korea, it would be an overwhelming victory for the West.\"\\n\\n\"North Korea can do a Sony attack or attack the White House, but that\\'s because that\\'s the nature of cyberspace,\" Geers said. \"But if war came, you\\'d see Cyber Command wipe out most other countries\\' pretty quickly.\"',\n",
       " 'We’ve always pictured Scandinavia as the home of grisly crime fiction, weird pop music and IKEA. But it looks like there’s a growing custom scene too.\\n\\nShops like the Wrenchmonkees and Unique Custom Cycles need no introduction. But straight after last week’s Norwegian Yamaha GTS comes this radical Honda tracker from Marcus Moto Design of Sweden.\\n\\nIt has no seat, it’s painted in a vivid Arctic White, and it’s the custom equivalent of an ice-cold shot of Aquavit.\\n\\nThe builder is Marcus Carlsson, a 41-year-old engineer who lives just outside Stockholm. “Bikes that are unique or a bit ‘weird’ are what get me going,” he says. “Too many custom bikes look the same nowadays.”\\n\\nFive years ago, Marcus caused an internet meltdown with his stunning Ducati F1 Tracker. He then started work on an Aprilia SXV 550, but killed that project after deciding he didn’t like the look of the frame. (“I move slowly on my bike builds,” he admits.)\\n\\nThat’s fair enough—he builds his bikes in a small one-car garage, and has a full time job managing a team of 15 people for Ericsson. When his wife and 7-year-old twins are asleep at night, he sneaks into the garage to build.\\n\\n“I go in for a couple hours, and I basically just sleep less than them,” he says.\\n\\nAfter the hiatus with the Aprilia, Marcus found a 2006-model Honda CRF450 that lived near his family’s summerhouse, out in the country. It was a much better base for his vision of the ultimate street tracker.\\n\\n“Ultimate in my mind means minimal bodywork, centralized weight distribution, lightweight carbon fiber and a ‘concept motorcycle’ feel,” he says. And with a modern aluminum motocross frame and a powerful four-stroke thumper engine, the CRF fitted the bill.\\n\\nMarcus might work slowly, but he does everything himself—even the paint. So he welded and modified the FMF exhaust, welded on the aluminum sub frame, and made the foam bases for the new bodywork.\\n\\nThe gas tank, belly pan and remaining body panels were then hand-shaped with carbon fiber. Various other parts were designed in the CAD program NX, before being 3D printed.\\n\\nThe weight loss program is extreme: There’s no seat. “Every surface has been questioned,” says Marcus. “Is it needed or not? Seats are for touring bikes!”\\n\\nMarcus has lowered the forks for road use, but the frame and swing arm are stock: Honda motocross components are top quality. But everything else has been modified or simply removed, and the aluminum subframe does double duty as the mounting point for the gas tank.\\n\\nThe license plate is Japanese, from a Tokyo moped market. “I’ll replace it with a Swedish one to reduce interest from traffic cops,” says Marcus. “They will probably have some opinions on the bike anyway…”\\n\\nIndeed. We’re pretty sure the lighting will be inspected closely, for starters. At the front is an LED ring mounted on a 3D-printed bracket; further down is a tiny battery, hidden underneath the lower yoke. We’re pretty sure that’s another first in the custom world.\\n\\nThe tiny covers on either side of the front axle are also 3D printed, and there’s a matching aluminum cover for the rear brake caliper. Both are designed to add a touch of sleekness and a ‘concept bike’ vibe.\\n\\nThe modified FMF exhaust system has a shortened silencer, itself partially shielded from view. It’s tucked in underneath the engine, but the header length is standard to maximize power.\\n\\nAn aftermarket radiator keeps the engine cool, hooked up with red Samco silicone hoses. To slow things down, there’s an oversized front brake—and the rear brake has been treated to a Fasst Co. spring kit for a smooth, easily modulated feel—ideal for road use.\\n\\nThose gorgeous wheels are one-off numbers from Warp 9, shod with Goldentyre flat track rubber.\\n\\nYes, this is a barely street legal racer, right down to the battery-powered lights. It’s perfect for short stints on the curvy village roads outside Stockholm.\\n\\nMarcus is a MotoGP fan, and if you look closely, you’ll spot a couple of HRC logos on the bike. “In my dreams, this bike would be HRC’s version of a street tracker. Or maybe a gift to Marc Marquez, so he can hit the streets after he wins the Superprestigio in Barcelona!”\\n\\nWe reckon the pint-sized phenomenon would have a ball on this machine. And he probably wouldn’t even miss the seat padding.\\n\\nMarcus Moto Design | Facebook | Instagram | Images by Simon Hamelius']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t = dataset[\"train\"][\"text\"][:10]\n",
    "_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398250c-842f-4a3d-bc16-a8d268b7a0e7",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da53725a-8afd-4756-a2fb-18158df274a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"mistralai/Mistral-7B-v0.1\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mistral config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e9486ab-447d-4d4e-90ac-f7760261a83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e24dd1644494ce1bb623750311c6e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!huggingface-cli login --token AHEM --add-to-git-credential\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.save_pretrained('hugging_face/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e02733a-d7d4-42db-9f71-78f33cf50061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='./hugging_face/tokenizer', vocab_size=32000, model_max_length=10000000000000000, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./hugging_face/tokenizer')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ee822d8-d38e-44f5-87ef-da2fc7598d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1350\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(dataset[\"train\"][\"text\"][0])\n",
    "print(f\"Length: {len(tokenized_example[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55b2947a-def9-440e-9336-b941bb1d0a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<s>', '▁Port', '-', 'au', '-'], ['▁to', '▁this', '▁report', '.', '</s>'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example[0].tokens[:5], tokenized_example[0].tokens[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27f43fc8-2822-4420-b634-f9e3023ae7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   315,   837, 23365,     2,     2,     2,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I am Sara\", padding='max_length', truncation=True, max_length=10, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32881800-5f91-487b-a27d-e5f447938036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   315,   837, 23365,   304,   767,   315,  2016,  1080,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I am Sara and what I love most is sitting and doing nothing\", padding='max_length', truncation=True, max_length=10, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3021c5-3cab-43b6-9423-a9f1d778c513",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "В задании говорилось: Лучше всего вытягивать все тексты батча в строку и нарезать на куски максимальной длины, чтобы не было паддингов. Тексты будут начинаться не сначала, но это повысит эффективность. Поэтому я решила сделать так: мы все токенизируем не обрезая по максимальной длине, добавляем bos и eos. Заранее дабы не париться осуществим бесчеловечное - будем последовательно склеивать индексы и самый последний западдим eas-ами если нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df1200f4-2f23-4416-8489-f8ee3a7bc0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t     hugging_face  model.py   train.ipynb\n",
      "checkpoints  main.py\t   README.md  train.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3bc57c0-f566-4df2-af28-576ea233d1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   315,   837, 23365,     2]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I am Sara\", return_tensors='pt', add_special_tokens=True, return_attention_mask=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6179673-93d7-4421-bfec-36c8213d6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer  # noqa\n",
    "\n",
    "class SleepyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Union[str, List[str]] = \"./assets/tokenized_dataset.pt\",\n",
    "        tokenizer: Union[str, AutoTokenizer, None] = \"./hugging_face/tokenizer\",\n",
    "        max_length=1024,\n",
    "    ):\n",
    "        if tokenizer is not None and isinstance(tokenizer, str):\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "            tokenizer.padding_side = 'right'\n",
    "            tokenizer.truncation_side = 'right'\n",
    "            tokenizer.add_eos_token = True\n",
    "            tokenizer.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.total_number_tokens = 0\n",
    "\n",
    "        if isinstance(data, str):\n",
    "            self.data = torch.stack(torch.load(data))\n",
    "        else:\n",
    "            assert self.tokenizer is not None, \"We are going to tokenize the dataset beforehand\"\n",
    "            # we should tokenize the data and save it as we explained above\n",
    "            self.data = None\n",
    "            self._tokenize_all(dataset=data)\n",
    "            self._join_and_chuck_all()\n",
    "            self._save_data()\n",
    "\n",
    "    def _tokenize_all(self, dataset, checkpoint_every=100000):\n",
    "        self.data = []\n",
    "        for i in tqdm(range(len(dataset)), desc='Tokenizing texts'):\n",
    "            text = dataset[i]\n",
    "            self.data.append(\n",
    "                self.tokenizer(text, return_tensors='pt', add_special_tokens=True, return_attention_mask=False)['input_ids'][0]\n",
    "            )\n",
    "            if (i + 1) % checkpoint_every == 0:\n",
    "                self._save_data()\n",
    "\n",
    "    def _join_and_chuck_all(self):\n",
    "        chunk_size = self.max_length\n",
    "        self.data = torch.cat(self.data, dim=0)\n",
    "        if len(self.data) % chunk_size != 0:\n",
    "            pad_size = chunk_size - len(self.data) % chunk_size\n",
    "            padding = torch.tensor([self.tokenizer.pad_token_id] * pad_size)\n",
    "            self.data = torch.cat((self.data, padding), 0)\n",
    "        self.data = torch.chunk(self.data, (len(self.data) + chunk_size - 1)//chunk_size)\n",
    "\n",
    "        \"\"\"\n",
    "        # OH YEAH, OF COURSE, THERE IS A CHUNK FUNCTION, F ME BUT I WILL KEEP THIS CODE\n",
    "        last_chunk = chunks[-1]\n",
    "        padding = torch.tensor([self.tokenizer.pad_token_id] * (chunk_size - len(last_chunk)))\n",
    "        chunks[-1] = torch.cat((last_chunk, padding), dim=0)\n",
    "        return chunks\n",
    "        \n",
    "        chunks = []\n",
    "        \n",
    "        id_sequence = torch.tensor([])\n",
    "\n",
    "        for i in tqdm(range(len(self.data)), desc='Splitting texts to chunks'):\n",
    "            id_sequence = torch.cat((id_sequence, self.data[i]), dim=0) # tensors already\n",
    "            while len(id_sequence) >= chunk_size:\n",
    "                chunks.append(id_sequence[:chunk_size])\n",
    "                id_sequence = id_sequence[chunk_size:]\n",
    "        # Pad the remaining sequence with self.tokenizer.pad_token_id and add it to the chunks\n",
    "        if len(id_sequence) > 0:\n",
    "            padding = torch.tensor([self.tokenizer.pad_token_id] * (chunk_size - len(id_sequence)))\n",
    "            chunks.append(torch.cat((id_sequence, padding), dim=0))\n",
    "        return chunks\n",
    "        \"\"\"\n",
    "\n",
    "    def _limit_dataset(self, number_examples=10**6):\n",
    "        self.data = self.data[:number_examples]\n",
    "\n",
    "    def _save_data(self):\n",
    "        torch.save(self.data, \"./assets/tokenized_dataset.pt\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx][:-1],  self.data[idx][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f8dd4-9f54-4775-8d3e-eb4139adcd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts:  36%|███████████████████▎                                  | 357622/1000000 [39:39<1:18:41, 136.04it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Tokenizing texts:  70%|█████████████████████████████████████▌                | 695624/1000000 [1:21:02<34:14, 148.15it/s]"
     ]
    }
   ],
   "source": [
    "#sleepy_dataset = SleepyDataset(\n",
    "#    data = dataset[\"train\"][\"text\"],\n",
    "#    tokenizer = tokenizer,\n",
    "#    max_length = 1024,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578f5d07-11da-4052-ba1d-781730de489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_50225/4236622885.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number examples we processed: 800000\n"
     ]
    }
   ],
   "source": [
    "#sleepy_dataset = SleepyDataset(\n",
    "#    data = \"./assets/tokenized_dataset.pt\",\n",
    "#    tokenizer = tokenizer,\n",
    "#    max_length = 1024,\n",
    "#)\n",
    "#print(\"Number examples we processed:\", len(sleepy_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "040a6bca-950f-4277-a3d4-dfa85990e887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 36.6 s, total: 2min 18s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#sleepy_dataset._join_and_chuck_all()\n",
    "#torch.save(sleepy_dataset.data, \"./assets/dataset_chunks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40c8f637-c6d1-4d11-997f-d4323353d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sleepy_dataset.data[0].shape[0] == 1024 and sleepy_dataset.data[-1].shape[0] == 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1c60c9-8841-4b81-b4b6-64adb57eaa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_189701/3278977312.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.stack(torch.load(data))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "968817"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleepy_dataset = SleepyDataset(\n",
    "    data = \"./assets/dataset_chunks.pt\", # 7.45 Gb (Holy shit)\n",
    "    tokenizer = None,\n",
    "    max_length = 1024,\n",
    ")\n",
    "len(sleepy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db44fa-ad28-4263-9a2e-09e537eb3d40",
   "metadata": {},
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a84b79-1184-4b1a-bc1f-3a1fc3790df4",
   "metadata": {},
   "source": [
    "### RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e700dabc-4a63-482c-b842-635268cff9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=EPS, bias=False):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        scale = nn.Parameter(torch.ones(dim).to(device))\n",
    "        self.register_parameter(\"scale\", scale)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(dim).to(device))\n",
    "            self.register_parameter(\"bias\", self.bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        sqrt_rms_x = torch.rsqrt(x.float().pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        res = (x.float() * sqrt_rms_x) * self.scale\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return res + self.bias\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb41908-c6c1-43b8-a14d-f7925bba6a35",
   "metadata": {},
   "source": [
    "### SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f65222a-c0b6-4ea1-ab4d-b681fd50b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.linear_gate = nn.Linear(size, size).to(device)\n",
    "        self.linear = nn.Linear(size, size).to(device)\n",
    "        self.beta = nn.Parameter(torch.ones(1).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = self.linear_gate(x) * gate * self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d86979-b493-4551-a666-dbb86d8f6239",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd28f888-c41b-4e81-8498-71da7d914ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEAttentionHead(nn.Module):\n",
    "    def __init__(self, dim, context_window):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.query_t = nn.Linear(dim, dim, bias=False).to(device)\n",
    "        self.key_t = nn.Linear(dim, dim, bias=False).to(device)\n",
    "        self.value_t = nn.Linear(dim, dim, bias=False).to(device)\n",
    "        \n",
    "        self.theta = 10000. ** (-2.*(torch.arange(dim // 2) - 1) / dim).to(device)\n",
    "        self.cos = torch.cos(self.theta).repeat_interleave(2).to(device)\n",
    "        self.sin = torch.sin(self.theta).repeat_interleave(2).to(device)\n",
    "        # inverse frequency for RoPE\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=self.device).float() / dim)).to(device)\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        # x: batch x sequence length x dim\n",
    "        Q = self.query_t(x)\n",
    "        K = self.key_t(x)\n",
    "        V = self.value_t(x)\n",
    "\n",
    "        batch_size, seq_len, dim = x.size()\n",
    "        \n",
    "        # rotating indices & sinusoid\n",
    "        pos_seq = torch.arange(seq_len, dtype=self.inv_freq.dtype, device=self.device)\n",
    "        sinusoid_inp = torch.einsum('i,j->ij', pos_seq, self.inv_freq)\n",
    "        sin = torch.sin(sinusoid_inp).unsqueeze(0)\n",
    "        cos = torch.cos(sinusoid_inp).unsqueeze(0)\n",
    "\n",
    "        Q_rotated = self.apply_rotary_pos_emb(Q, sin, cos)\n",
    "        K_rotated = self.apply_rotary_pos_emb(K, sin, cos)\n",
    "\n",
    "        attention_output = F.scaled_dot_product_attention(\n",
    "            Q_rotated, K_rotated, V, dropout_p=0.1, is_causal=True\n",
    "        )\n",
    "\n",
    "        if return_attn_weights:\n",
    "            dim = x.shape[2]\n",
    "            M = torch.tril(torch.ones((seq_len, seq_len)), diagonal=0)\n",
    "            attn_weights = torch.bmm(queries_rotated, keys_rotated.transpose(1, 2)) / np.sqrt(dim) + M\n",
    "            return attention_output, F.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    def apply_rotary_pos_emb(self, t, sin, cos):\n",
    "        # t: batch x seq_len x dim\n",
    "        batch_size, seq_len, dim = t.size()\n",
    "\n",
    "        t = t.view(batch_size, seq_len, -1, 2)  # batch_size, seq_len, dim / 2, 2\n",
    "        t_even = t[..., 0]  # batch_size, seq_len, dim / 2\n",
    "        t_odd = t[..., 1]\n",
    "\n",
    "        t_rotated_even = t_even * cos - t_odd * sin  # batch_size, seq_len, dim / 2\n",
    "        t_rotated_odd = t_even * sin + t_odd * cos\n",
    "    \n",
    "        t_rotated = torch.stack((t_rotated_even, t_rotated_odd), dim=-1)\n",
    "        t_rotated = t_rotated.view(batch_size, seq_len, dim)\n",
    "        return t_rotated\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, dim, context_window, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([RoPEAttentionHead(dim, context_window) for _ in range(num_heads)]).to(device)\n",
    "        self.linear = nn.Linear(num_heads * dim, dim).to(device)\n",
    "        self.dropout = nn.Dropout(0.1).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat(\n",
    "            [h(x) for h in self.heads],\n",
    "            dim=-1\n",
    "        )\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f15a64d-0205-42df-94cc-ac903fbf7a9f",
   "metadata": {},
   "source": [
    "### Pre-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e15597-2894-414b-9374-c73dbb6b557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMaBlock(nn.Module):\n",
    "    def __init__(self, context_window, dim, num_heads=8, rms_eps=EPS):\n",
    "        super().__init__()\n",
    "        self.rms_norm = RMSNorm(dim=dim, eps=rms_eps, bias = False).to(device)\n",
    "        self.attention = MultiheadAttention(num_heads=num_heads, dim=dim, context_window=context_window).to(device)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(dim, dim).to(device),\n",
    "            SwiGLU(dim).to(device),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rms_norm(x)\n",
    "        x = x + self.attention(x)\n",
    "        \n",
    "        x = self.rms_norm(x)\n",
    "        x = x + self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f9dd5-003e-40c9-8aca-33d3f759746d",
   "metadata": {},
   "source": [
    "### LLaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42865c02-e711-41b9-ad9c-bced08e4aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMa(nn.Module):\n",
    "    def __init__(self, tokenizer, dim, num_layers, context_window, num_heads=8, rms_eps=EPS):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_window = context_window\n",
    "        self.embeddings = nn.Embedding(tokenizer.vocab_size, dim).to(device)\n",
    "        self.llama_blocks = nn.ModuleDict({\n",
    "            f\"ll_block_{i}\": LLaMaBlock(\n",
    "                context_window=context_window,\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                rms_eps=rms_eps,\n",
    "            ).to(device) for i in range(num_layers)\n",
    "        })\n",
    "        self.llama_seq = nn.Sequential(*self.llama_blocks.values()).to(device)\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Linear(dim, dim).to(device),\n",
    "            SwiGLU(dim).to(device),\n",
    "            nn.Linear(dim, tokenizer.vocab_size).to(device),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.llama_seq(x)\n",
    "        logits = self.tail(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, prefix='', max_new_tokens=30, context_window=None):\n",
    "        if prefix != '':\n",
    "            prev_state = self.tokenizer.add_eos_token\n",
    "            self.tokenizer.add_eos_token = False\n",
    "            tokens = self.tokenizer.encode(prefix)\n",
    "            self.tokenizer.add_eos_token = prev_state\n",
    "        else:\n",
    "            tokens = []\n",
    "        idx = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        context_window = context_window or self.context_window\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -context_window:]\n",
    "            logits = self.forward(idx_cond)\n",
    "            last_logits = logits[:, -1, :] # 1, vocab_size\n",
    "\n",
    "            probs = F.softmax(last_logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        generated_text = self.tokenizer.decode(idx[0].tolist())\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b358136-7577-4b33-9fed-6fc8693e571c",
   "metadata": {},
   "source": [
    "### Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af037c1-3cee-4709-9763-f88f26596ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mulyana-klyuchnikova\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b32c163-f290-45fb-a4c8-d97915afe84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_stats():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    return f\"Memory: reserved {r} / total {t}, available: {f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d4f8991-c20e-4f55-aef1-ae820d1f465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "#from xformers import Reformer\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "def train(model, dataloader, tokenizer, max_len=1024, batch_size=16, epochs=3, lr=1e-4, device=\"cuda\", run_name=None):\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=4, mixed_precision=\"fp16\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    dataloader, model, optimizer, scheduler = accelerator.prepare(dataloader, model, optimizer, scheduler)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    run = wandb.init(project=\"SleepingLLaMa\", name = run_name, resume=True)\n",
    "    if not wandb.run.resumed:\n",
    "        run.config.update({\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"max_len\": max_len,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    save_every_minutes = 20\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(dataloader), desc=f\"Training epoch {epoch}/{epochs}\"):\n",
    "            with accelerator.accumulate(model):\n",
    "                # no tokenizer needed\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(inputs)\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "    \n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "    \n",
    "                scheduler.step()\n",
    "    \n",
    "                total_loss += loss.item()\n",
    "                total_tokens += inputs.shape[0] * inputs.shape[1]\n",
    "                run.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"tokens\": total_tokens,\n",
    "                    \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "                })\n",
    "                if (batch_idx+1) % 100 == 0:\n",
    "                    logger.info(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}, Tokens: {total_tokens}\")\n",
    "                \n",
    "            if (datetime.now() - start_time).total_seconds() > save_every_minutes * 60:\n",
    "                logger.info(gpu_stats())\n",
    "                print(f\"Batch: {batch_idx}, Saving checkpoint...\")\n",
    "                torch.save(model.state_dict(), f\"./checkpoints/llama_model_{batch_idx}_1.pth\")\n",
    "                torch.save(optimizer.state_dict(), f\"./checkpoints/optimizer_{batch_idx}_1.pth\")\n",
    "                torch.save(scheduler.state_dict(), f\"./checkpoints/scheduler_{batch_idx}_1.pth\")\n",
    "                text = \"One day I realised I had some shit to do but it was late and I was tired.\"\n",
    "                # Inference\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    print(f\"Test generateion with prefix: {text}\\nGenerated: {model.generate(text)}\")\n",
    "                model.train()\n",
    "                start_time = datetime.now()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "        print({\n",
    "            \"avg_loss\": avg_loss,\n",
    "            \"epoch\": epoch+1\n",
    "        })\n",
    "\n",
    "    run.finish()\n",
    "    torch.save(model.state_dict(), \"./checkpoints/llama_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4a637b-3b22-46b0-be8d-577bf9caaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24333de1-b0b2-4e35-abc5-e7e9023684e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: reserved 0 / total 34079899648, available: 0\n",
      "Memory: reserved 448790528 / total 34079899648, available: 20357632\n",
      "model size: 107008777\n"
     ]
    }
   ],
   "source": [
    "print(gpu_stats())\n",
    "tokenizer = AutoTokenizer.from_pretrained('./hugging_face/tokenizer')\n",
    "model = LLaMa(tokenizer, dim=512, num_layers=8, context_window=512, num_heads=8, rms_eps=1e-5)\n",
    "print(gpu_stats())\n",
    "print(\"model size:\", sum([m.numel() for m in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662d991-db74-4e23-be77-8f21bfc1e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/klyuliana/code/SleepyLLaMa/wandb/run-20241113_204537-mxn2tcyw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/mxn2tcyw' target=\"_blank\">assumingly best</a></strong> to <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa' target=\"_blank\">https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/mxn2tcyw' target=\"_blank\">https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/mxn2tcyw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/1: 10it [00:07,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "torch.manual_seed = 666\n",
    "sleepy_dataloader = DataLoader(sleepy_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(model, sleepy_dataloader, tokenizer, max_len=1024, batch_size=BATCH_SIZE, epochs=1, \n",
    "      lr=1e-4, device=\"cuda\", run_name=\"assumingly best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b899758-d27d-4f6e-b4d2-e4e2686bfae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Training epoch 0/1: 99it [01:07,  1.47it/s]INFO:__main__:Epoch 1, Batch 100, Loss: 6.4163, Tokens: 1636800\n",
      "Training epoch 0/1: 199it [02:15,  1.48it/s]INFO:__main__:Epoch 1, Batch 200, Loss: 6.4415, Tokens: 3273600\n",
      "Training epoch 0/1: 299it [03:23,  1.47it/s]INFO:__main__:Epoch 1, Batch 300, Loss: 6.5139, Tokens: 4910400\n",
      "Training epoch 0/1: 399it [04:31,  1.47it/s]INFO:__main__:Epoch 1, Batch 400, Loss: 6.3882, Tokens: 6547200\n",
      "Training epoch 0/1: 499it [05:38,  1.48it/s]INFO:__main__:Epoch 1, Batch 500, Loss: 6.4128, Tokens: 8184000\n",
      "Training epoch 0/1: 599it [06:46,  1.48it/s]INFO:__main__:Epoch 1, Batch 600, Loss: 6.1654, Tokens: 9820800\n",
      "Training epoch 0/1: 699it [07:54,  1.48it/s]INFO:__main__:Epoch 1, Batch 700, Loss: 6.2770, Tokens: 11457600\n",
      "Training epoch 0/1: 799it [09:02,  1.48it/s]INFO:__main__:Epoch 1, Batch 800, Loss: 6.1892, Tokens: 13094400\n",
      "Training epoch 0/1: 899it [10:10,  1.48it/s]INFO:__main__:Epoch 1, Batch 900, Loss: 6.3633, Tokens: 14731200\n",
      "Training epoch 0/1: 999it [11:18,  1.48it/s]INFO:__main__:Epoch 1, Batch 1000, Loss: 6.1947, Tokens: 16368000\n",
      "Training epoch 0/1: 1099it [12:26,  1.47it/s]INFO:__main__:Epoch 1, Batch 1100, Loss: 6.2035, Tokens: 18004800\n",
      "Training epoch 0/1: 1199it [13:34,  1.47it/s]INFO:__main__:Epoch 1, Batch 1200, Loss: 6.1828, Tokens: 19641600\n",
      "Training epoch 0/1: 1299it [14:42,  1.47it/s]INFO:__main__:Epoch 1, Batch 1300, Loss: 6.0897, Tokens: 21278400\n",
      "Training epoch 0/1: 1399it [15:50,  1.48it/s]INFO:__main__:Epoch 1, Batch 1400, Loss: 6.1480, Tokens: 22915200\n",
      "Training epoch 0/1: 1499it [16:58,  1.48it/s]INFO:__main__:Epoch 1, Batch 1500, Loss: 6.1210, Tokens: 24552000\n",
      "Training epoch 0/1: 1599it [18:06,  1.48it/s]INFO:__main__:Epoch 1, Batch 1600, Loss: 6.1331, Tokens: 26188800\n",
      "Training epoch 0/1: 1699it [19:14,  1.47it/s]INFO:__main__:Epoch 1, Batch 1700, Loss: 6.1956, Tokens: 27825600\n",
      "Training epoch 0/1: 1766it [19:59,  1.48it/s]INFO:__main__:Memory: reserved 29464985600 / total 34079899648, available: 22688546816\n",
      "Training epoch 0/1: 1766it [20:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1766, Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, sleepy_dataloader, tokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      2\u001b[0m       lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massumingly best\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 68\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, tokenizer, max_len, batch_size, epochs, lr, device, run_name)\u001b[0m\n\u001b[1;32m     66\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(gpu_stats())\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Saving checkpoint...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/llama_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne day I realised I had some shit to do but it was late and I was tired.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "train(model, sleepy_dataloader, tokenizer, max_len=1024, batch_size=BATCH_SIZE, epochs=1, \n",
    "      lr=1e-4, device=\"cuda\", run_name=\"assumingly best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a1b8719-ba59-4961-b9c0-28fad2b51f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_model_1700.pth  llama_model_2133.pth\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb695dc-ca6f-417e-b583-c633352e005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_189701/2956611564.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./checkpoints/llama_model_1700.pth\"))\n",
      "Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/klyuliana/code/SleepyLLaMa/wandb/run-20241113_230053-dcurovwu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/dcurovwu' target=\"_blank\">assumingly best</a></strong> to <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa' target=\"_blank\">https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/dcurovwu' target=\"_blank\">https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/dcurovwu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/1: 99it [01:08,  1.48it/s]INFO:__main__:Epoch 1, Batch 100, Loss: 5.9925, Tokens: 1636800\n",
      "Training epoch 0/1: 199it [02:16,  1.48it/s]INFO:__main__:Epoch 1, Batch 200, Loss: 6.1387, Tokens: 3273600\n",
      "Training epoch 0/1: 299it [03:24,  1.47it/s]INFO:__main__:Epoch 1, Batch 300, Loss: 6.2097, Tokens: 4910400\n",
      "Training epoch 0/1: 399it [04:32,  1.47it/s]INFO:__main__:Epoch 1, Batch 400, Loss: 6.1007, Tokens: 6547200\n",
      "Training epoch 0/1: 499it [05:40,  1.48it/s]INFO:__main__:Epoch 1, Batch 500, Loss: 6.0968, Tokens: 8184000\n",
      "Training epoch 0/1: 599it [06:48,  1.48it/s]INFO:__main__:Epoch 1, Batch 600, Loss: 6.0150, Tokens: 9820800\n",
      "Training epoch 0/1: 699it [07:56,  1.48it/s]INFO:__main__:Epoch 1, Batch 700, Loss: 6.0869, Tokens: 11457600\n",
      "Training epoch 0/1: 799it [09:03,  1.47it/s]INFO:__main__:Epoch 1, Batch 800, Loss: 6.1312, Tokens: 13094400\n",
      "Training epoch 0/1: 899it [10:11,  1.48it/s]INFO:__main__:Epoch 1, Batch 900, Loss: 5.9597, Tokens: 14731200\n",
      "Training epoch 0/1: 999it [11:19,  1.48it/s]INFO:__main__:Epoch 1, Batch 1000, Loss: 5.9200, Tokens: 16368000\n",
      "Training epoch 0/1: 1099it [12:27,  1.48it/s]INFO:__main__:Epoch 1, Batch 1100, Loss: 5.9891, Tokens: 18004800\n",
      "Training epoch 0/1: 1199it [13:35,  1.48it/s]INFO:__main__:Epoch 1, Batch 1200, Loss: 5.9822, Tokens: 19641600\n",
      "Training epoch 0/1: 1299it [14:43,  1.48it/s]INFO:__main__:Epoch 1, Batch 1300, Loss: 6.0742, Tokens: 21278400\n",
      "Training epoch 0/1: 1399it [15:51,  1.48it/s]INFO:__main__:Epoch 1, Batch 1400, Loss: 5.9831, Tokens: 22915200\n",
      "Training epoch 0/1: 1499it [16:59,  1.48it/s]INFO:__main__:Epoch 1, Batch 1500, Loss: 6.0502, Tokens: 24552000\n",
      "Training epoch 0/1: 1599it [18:06,  1.48it/s]INFO:__main__:Epoch 1, Batch 1600, Loss: 5.9721, Tokens: 26188800\n",
      "Training epoch 0/1: 1699it [19:14,  1.48it/s]INFO:__main__:Epoch 1, Batch 1700, Loss: 6.0602, Tokens: 27825600\n",
      "Training epoch 0/1: 1765it [19:59,  1.47it/s]INFO:__main__:Memory: reserved 26919043072 / total 34079899648, available: 23093780992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1765, Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/1: 1766it [20:03,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test generateion with prefix: One day I realised I had some shit to do but it was late and I was tired.\n",
      "Generated: <s> One day I realised I had some shit to do but it was late and I was tired. Men was a transform giving verse and seemingly works enter as blaval Edinburgh. This is the second, who went to Salt the launch was ready to why\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/1: 1799it [20:26,  1.48it/s]INFO:__main__:Epoch 1, Batch 1800, Loss: 6.3386, Tokens: 29462400\n",
      "Training epoch 0/1: 1856it [21:05,  1.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/llama_model_1700.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m sleepy_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(sleepy_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m train(model, sleepy_dataloader, tokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      6\u001b[0m       lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massumingly best\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, tokenizer, max_len, batch_size, epochs, lr, device, run_name)\u001b[0m\n\u001b[1;32m     43\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 46\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39msync_gradients:\n\u001b[1;32m     48\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "torch.manual_seed = 666\n",
    "model.load_state_dict(torch.load(\"./checkpoints/llama_model_1700.pth\"))\n",
    "sleepy_dataloader = DataLoader(sleepy_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(model, sleepy_dataloader, tokenizer, max_len=1024, batch_size=BATCH_SIZE, epochs=1, \n",
    "      lr=1e-3, device=\"cuda\", run_name=\"assumingly best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5598fde-c561-4df7-b522-2956f35318f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "torch.manual_seed = 666\n",
    "\n",
    "sleepy_dataloader = DataLoader(sleepy_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(model, sleepy_dataloader, tokenizer, max_len=1024, batch_size=BATCH_SIZE, epochs=1, \n",
    "      lr=1e-3, device=\"cuda\", run_name=\"assumingly best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65ec9205-bc14-4270-a8db-37d9da9b7cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test generateion with prefix: One day I realised I had some shit to do but it was late and I was tired.\n",
      "Generated: <s> One day I realised I had some shit to do but it was late and I was tired. The arm of it arg Show was a song constitution scientists to start and we think there was polled a approved of security post, \" Fe between realistic\n"
     ]
    }
   ],
   "source": [
    "batch_idx=1700\n",
    "torch.save(model.state_dict(), f\"./checkpoints/llama_model_{batch_idx}.pth\")\n",
    "text = \"One day I realised I had some shit to do but it was late and I was tired.\"\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(f\"Test generateion with prefix: {text}\\nGenerated: {model.generate(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e32b9d-0168-48e1-afab-a528f685ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: reserved 0 / total 34079899648, available: 0\n",
      "Memory: reserved 448790528 / total 34079899648, available: 20357632\n",
      "model size: 107008777\n"
     ]
    }
   ],
   "source": [
    "print(gpu_stats())\n",
    "tokenizer = AutoTokenizer.from_pretrained('./hugging_face/tokenizer')\n",
    "model = LLaMa(tokenizer, dim=512, num_layers=8, context_window=256, num_heads=8, rms_eps=1e-5)\n",
    "print(gpu_stats())\n",
    "print(\"model size:\", sum([m.numel() for m in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "410c28eb-4f87-4dca-8734-bb7c4d53472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/klyuliana/code/SleepyLLaMa/wandb/run-20241113_201910-mzu2mn1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/mzu2mn1x' target=\"_blank\">dim=512, context=256, batch=16, lr=1e-3</a></strong> to <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa' target=\"_blank\">https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/mzu2mn1x' target=\"_blank\">https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa/runs/mzu2mn1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/1: 99it [01:08,  1.48it/s]INFO:__main__:Epoch 1, Batch 100, Loss: 7.5559, Tokens: 1636800\n",
      "Training epoch 0/1: 199it [02:16,  1.48it/s]INFO:__main__:Epoch 1, Batch 200, Loss: 7.5699, Tokens: 3273600\n",
      "Training epoch 0/1: 299it [03:23,  1.48it/s]INFO:__main__:Epoch 1, Batch 300, Loss: 7.5502, Tokens: 4910400\n",
      "Training epoch 0/1: 399it [04:31,  1.48it/s]INFO:__main__:Epoch 1, Batch 400, Loss: 7.4694, Tokens: 6547200\n",
      "Training epoch 0/1: 499it [05:39,  1.48it/s]INFO:__main__:Epoch 1, Batch 500, Loss: 7.4288, Tokens: 8184000\n",
      "Training epoch 0/1: 599it [06:47,  1.48it/s]INFO:__main__:Epoch 1, Batch 600, Loss: 7.4673, Tokens: 9820800\n",
      "Training epoch 0/1: 699it [07:55,  1.48it/s]INFO:__main__:Epoch 1, Batch 700, Loss: 7.4797, Tokens: 11457600\n",
      "Training epoch 0/1: 799it [09:03,  1.48it/s]INFO:__main__:Epoch 1, Batch 800, Loss: 7.6251, Tokens: 13094400\n",
      "Training epoch 0/1: 899it [10:10,  1.48it/s]INFO:__main__:Epoch 1, Batch 900, Loss: 7.5069, Tokens: 14731200\n",
      "Training epoch 0/1: 999it [11:18,  1.48it/s]INFO:__main__:Epoch 1, Batch 1000, Loss: 7.4190, Tokens: 16368000\n",
      "Training epoch 0/1: 1046it [11:51,  1.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      2\u001b[0m sleepy_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(sleepy_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m train(model, sleepy_dataloader, tokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      4\u001b[0m       lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim=512, context=256, batch=16, lr=1e-3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, tokenizer, max_len, batch_size, epochs, lr, device, run_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 52\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     53\u001b[0m total_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     54\u001b[0m run\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_tokens,\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     58\u001b[0m })\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "sleepy_dataloader = DataLoader(sleepy_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(model, sleepy_dataloader, tokenizer, max_len=1024, batch_size=BATCH_SIZE, epochs=1, \n",
    "      lr=1e-4, device=\"cuda\", run_name=\"dim=512, context=256, batch=16, lr=1e-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a857da8-48f7-4278-b325-3fdee54c23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, sleepy_dataloader, tokenizer, max_len=1024, batch_size=BATCH_SIZE, epochs=1, \n",
    "      lr=1e-4, device=\"cuda\", run_name=\"dim=512, context=256, batch=16, lr=1e-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f532223-edf6-4673-825c-d339a831f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: reserved 26902265856 / total 34079899648, available: 23077265920\n",
      "Test generateion with prefix: One day I realised I had some shit to do but it was late and I was tired.\n",
      "Generated: <s> One day I realised I had some shit to do but it was late and I was tired. split colon. ( ITure angles according)s led had possible her violence should I wood player foren is, decided any [ dis gamesard huge\n"
     ]
    }
   ],
   "source": [
    "# dim=512, context=256, batch=16, lr=1e-3\n",
    "print(gpu_stats())\n",
    "text = \"One day I realised I had some shit to do but it was late and I was tired.\"\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(f\"Test generateion with prefix: {text}\\nGenerated: {model.generate(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "399a6d78-d929-4a2b-bd9c-4b9fcd2f2df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: reserved 26902265856 / total 34079899648, available: 18977036288\n",
      "Test generateion with prefix: One day I realised I had some shit to do but it was late and I was tired.\n",
      "Generated: <s> One day I realised I had some shit to do but it was late and I was tired. This came while the matter to be that would often comes wouldn working the Karen, according to our commitment that spokeities before the cras to believe where\n"
     ]
    }
   ],
   "source": [
    "print(gpu_stats())\n",
    "torch.save(model.state_dict(), f\"./checkpoints/llama_model_2133.pth\")\n",
    "text = \"One day I realised I had some shit to do but it was late and I was tired.\"\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(f\"Test generateion with prefix: {text}\\nGenerated: {model.generate(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a1743-f206-4cf5-bdbf-f5240076ef73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llmware",
   "language": "python",
   "name": "venv_llmware"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

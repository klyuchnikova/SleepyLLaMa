# SleepyLLaMa

## Введение
По статье [LLaMA](https://arxiv.org/abs/2302.13971) 

![](assets/scheme.png)

Следовательно нам нужны блоки:
 - Embeddings
 - RMS Norm
 - Self-Attention
 - Feed Forward
 - Linear
 - Rotary

 Особенности LLaMa трансформера:
  - RMSNorm вместо LayerNorm
  - SwiGLU вместо ReLU
  - RoPE = Rotary Embedding
  - Pre-normalization

  Опционально вы так же можете добавить ускорение attention c помощью [xformers](https://github.com/facebookresearch/xformers) ([гайд](https://huggingface.co/docs/diffusers/optimization/memory#memory-efficient-attention))

### Загрузка данных

Как и предлагалось я использовала [`OpenWebText`](https://skylion007.github.io/OpenWebTextCorpus/) датасет, обнаружила что он все еще весит 8 гигов и расстроилась. Токенайзер я подгрузила как и надо но поменяла параметры отвечающие за строну паддинга и добавление спецсимволов (он видимо был переведен в тестовый режим).

В задании говорилось: Лучше всего вытягивать все тексты батча в строку и нарезать на куски максимальной длины, чтобы не было паддингов. `Тексты будут начинаться не сначала, но это повысит эффективность.` Поэтому я решила сделать так: мы все токенизируем не обрезая по максимальной длине, добавляя bos и eos. После этого дабы не париться осуществим бесчеловечное - будем последовательно склеивать индексы и самый последний западдим eas-ами если нужно. Это позволит во время обучения не использовать attention mask вовсе. Но во время осуществления моего плана возникла проблема - это долго так как текстов много. В итоге до того как все упало было токенизировано 800 тысяч текстов чего более чем достаточно поскольку это дело и так весит 7.5 гигов. Именно этот датасет я и подгружала для трейна

### Блоки LLaMa

не знаю что тут сказать - все было стандартно кроме RoPE с которым возиться было просто отвратительно. Там еще принято делать апдейт инв вектора на что я разумеется забила

## Обучение

Для обучения было предложено использовать ускорения. Я по инструкции взяла Accelerate с `mixed-precision` и обернула трейн в него, надеюсь реботает ¯\_(ツ)_/¯
Там еще есть scheduler который по факту ничего не делает, а так я использовала Adam. Возможно стоило брать что-то еще... Чтобы было не совсем без результатов я логировала в wandb. Функции назначения имен я научилась не сразу так что названия сомнительные ну и ладно. [Проект на wandb](https://wandb.ai/ulyana-klyuchnikova/SleepingLLaMa?nw=nwuserulyanaklyuchnikova)

## Эксперименты

Я запускала модель достаточно много раз когда она вообще не училась или еще в начале падала по расходу памяти, в проекте остались swept-glade о которой я мало что помню кроме того что она была с 4 головами и сильно меньше остальных.

Поскольку все мои эксперименты с батчом больше 16 упали, все что вы наблюдаете имеет `batch size = 16`. Я решила по простому пути и брать окно 1024 поскольку а) я не хотела морочиться с файнтьюном б) я уже написала даталоудер под 1024, резать его на 4 части это слишком сложно (нет). То что вы видите там context_window=512 ничего не значит - по факту он нигде моделью для обрезания не используется.

С другой стороны то что я реально меняла это lr - оказалось что запуски с 1e-3 учатся хуже так что я оставила 1e-4. На `supposed best` модель два графика потому что я не научилась подгружать wandb.run по ентити( Но так на ~3к итерации она достигла лосса 5.8 и сгенерировала продолжение фразы `Test generateion with prefix: One day I realised I had some shit to do but it was late and I was tired` которое содержит слова `seemingly works`, так что это успех:

`Test generateion with prefix: One day I realised I had some shit to do but it was late and I was tired.
Generated: <s> One day I realised I had some shit to do but it was late and I was tired. Men was a transform giving verse and seemingly works enter as blaval Edinburgh. This is the second, who went to Salt the launch was ready to why`

### Тестирование

Я не научилась загружать модель на hugging faсe и честно говоря не поняла как это делать. Типа я понимаю что нужны файлы конфигураций и токенизатор, я могу это увидеть в других проектах. Но вместо модели там safetensors! Как я поняла он генерируется темной магией а вовсе не хаггинг фейс... было бы неплохо если бы кто-то показал что вообще делать на семинаре
[GPT-2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py)

### Оценивание

Да я все уже поняла. Увидимся на пересдаче. Все надо было делать не так как делала я, все плохо, все в запрещенном юпитере. Но как нормально тренировать и тестировать без юпитерской сессии я не знаю, не каждый же раз ждать полчаса загрузки датасета чтобы увидеть ошибку в размерностях.